{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMTh8q3mw+nKz5FrYhgt49N",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Pavithra777/Assignment2.5/blob/main/Assignment2_5.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IIu5RbX_ci_1"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data import Dataset"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Loading taining dataset from MNIST datasets\n",
        "train_set = torchvision.datasets.MNIST(\n",
        "    root = './data',\n",
        "    train = True,\n",
        "    download = True,\n",
        "    transform = transforms.Compose([\n",
        "        transforms.ToTensor()\n",
        "    ])\n",
        ")"
      ],
      "metadata": {
        "id": "erRo3Rcjdhvf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_set"
      ],
      "metadata": {
        "id": "ET4aYbzEenvS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e6a51803-1a0d-48c2-d956-dcfd95c43f0c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Dataset MNIST\n",
              "    Number of datapoints: 60000\n",
              "    Root location: ./data\n",
              "    Split: Train\n",
              "    StandardTransform\n",
              "Transform: Compose(\n",
              "               ToTensor()\n",
              "           )"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torchvision import datasets, transforms\n",
        "\n",
        "# This class add random input to the MNIST dataset\n",
        "class MNISTWithExtraInput(torch.utils.data.Dataset):\n",
        "    def __init__(self, root, extra_input_data, transform=None):\n",
        "        self.root = root\n",
        "        self.transform = transform\n",
        "        self.extra_input_data = extra_input_data\n",
        "\n",
        "        # Load the MNIST dataset\n",
        "        self.mnist_dataset = datasets.MNIST(root=self.root, train=True, download=True, transform=self.transform)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.mnist_dataset)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # Get the MNIST data and labels for the given index\n",
        "        x, y = self.mnist_dataset[idx]\n",
        "\n",
        "        # Get the extra input data for the given index\n",
        "        extra_input = self.extra_input_data[idx]\n",
        "\n",
        "        return x, y, extra_input\n"
      ],
      "metadata": {
        "id": "1JcYOX-g_1nz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torchvision import datasets, transforms\n",
        "\n",
        "# Combining List of MNISTWithExtraInput dataset \n",
        "class CombinedDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, datasets):\n",
        "        self.datasets = datasets\n",
        "\n",
        "    def __len__(self):\n",
        "        # Return the total number of elements in all datasets\n",
        "        return sum(len(dataset) for dataset in self.datasets)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # Find the dataset and index for the given element\n",
        "        for dataset in self.datasets:\n",
        "            if idx < len(dataset):\n",
        "                return dataset[idx]\n",
        "            idx -= len(dataset)"
      ],
      "metadata": {
        "id": "TwymvhrhGCYH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset_list = []\n",
        "# Adding random inputs ranging from 0 to 9 \n",
        "for i in range(0,10):\n",
        "  zero_tensor = x = torch.zeros(60000,1)\n",
        "  transform = transforms.Compose([\n",
        "        transforms.ToTensor()\n",
        "    ])\n",
        "  mnist_dataset = MNISTWithExtraInput('./data', zero_tensor+i,transform=transform)\n",
        "  dataset_list.append(mnist_dataset)\n",
        "# Merging all 10 MNISTWithExtraInput \n",
        "combined_dataset=CombinedDataset(dataset_list)"
      ],
      "metadata": {
        "id": "m6PMdMHbGJFO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating dataloader of batch size 64\n",
        "train_loader = torch.utils.data.DataLoader(combined_dataset, batch_size=64, shuffle=True)\n"
      ],
      "metadata": {
        "id": "GoAbJ30UVvWr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_loader.dataset[0]\n",
        "train_loader.dataset[8]"
      ],
      "metadata": {
        "id": "sj1qfgOeIxbF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "class Network(nn.Module):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "    self.conv1 = nn.Conv2d(in_channels=1, out_channels= 32,kernel_size=3)  # 1x28x28|32x3x3|32x26x26\n",
        "    self.conv2 = nn.Conv2d(in_channels=32, out_channels= 64,kernel_size=3) # 32x26x26|64x3x3|64x24x24\n",
        "    self.conv3 = nn.Conv2d(in_channels=64, out_channels= 128,kernel_size=3) # 64x12x12|128x3x3|128x10x10\n",
        "    self.out = nn.Linear(128*10*10,10)  # First output layer has 10 class  \n",
        "    self.out1 = nn.Linear(20,40) \n",
        "    self.out2 = nn.Linear(40,20) # Second output layer has 20 class (0 to 19)\n",
        "   \n",
        "  def forward(self,t,y):\n",
        "    # conv1 layer \n",
        "    x = self.conv1(t)\n",
        "    x= F.relu(x)\n",
        "    # conv2 layer \n",
        "    x = self.conv2(x)\n",
        "    x= F.relu(x)\n",
        "    x = F.max_pool2d(x,kernel_size=2,stride=2)\n",
        "    # conv3 layer \n",
        "    x = self.conv3(x)\n",
        "    x= F.relu(x)\n",
        "    x= x.reshape(64,-1)\n",
        "    x = self.out(x)\n",
        "    y= y.reshape(64,-1)\n",
        "    #Merging first output layer with second input layer\n",
        "    comb = torch.concat([x,y],axis = 1)\n",
        "    x1 = self.out1(comb) \n",
        "    x2 = self.out2(x1) \n",
        "    return x,x2\n",
        "   "
      ],
      "metadata": {
        "id": "vpjK_ZQSmAL7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_num_correct(preds,labels):\n",
        "  return preds.argmax(dim=1).eq(labels).sum().item()"
      ],
      "metadata": {
        "id": "vnD7zqizDBpq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.optim as optim\n",
        "network = Network()\n",
        "train_loader = torch.utils.data.DataLoader(combined_dataset, batch_size=64, shuffle=True)\n",
        "use_cuda = torch.cuda.is_available()\n",
        "device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
        "model = Network().to(device)\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "\n",
        "\n",
        "for t in range(0,5):\n",
        "  total_loss = 0\n",
        "  total_correct1 =0\n",
        "  total_correct2 =0\n",
        "  for batch in train_loader:\n",
        "    images,labels,num = batch\n",
        "    # moving all the input tensor to GPU\n",
        "    images,labels , num = images.to(device),labels.to(device),num.to(device)\n",
        "    random_num = num\n",
        "    # Converting random number (integer) to binary column\n",
        "    num = F.one_hot(num.to(torch.int64), num_classes = 10)\n",
        "    # Calling neural network which takes first input as image and second input as random number\n",
        "    #returns 2 output, First one is number detected on image\n",
        "    # and second one is sum of number on image and random number \n",
        "    pred1,pred2 = model(images,num)\n",
        "    #Calculating loss for first prediction\n",
        "    loss1 = F.cross_entropy(pred1.squeeze(),labels)\n",
        "    #Creating label for second prediction (sum of first output and second input)\n",
        "    label2 = labels + random_num.squeeze() \n",
        "    #Calculating loss for second prediction\n",
        "    #Using Cross-entropy loss Function  \n",
        "    #It measures the difference between the predicted probability distribution and the true probability distribution of the classes.\n",
        "    loss2 = F.cross_entropy(pred2.squeeze(),label2.long())\n",
        "    #Calculating total loss\n",
        "    loss = loss1+loss2\n",
        "    #zeroing the gradients from the previous step\n",
        "    optimizer.zero_grad()\n",
        "    #computing the gradients of the loss with respect to the model's parameters\n",
        "    loss.backward()\n",
        "    # updating the model's parameters based on these gradients\n",
        "    optimizer.step()\n",
        "    total_loss +=loss\n",
        "    # Gets total number of correct output\n",
        "    correct1 = get_num_correct(pred1,labels)\n",
        "    correct2 = get_num_correct(pred2,label2)\n",
        "    total_correct1 += correct1 \n",
        "    total_correct2 += correct2\n",
        "  print(\"epoch : \" , t , \"total loss : \", total_loss.item(), \" correct1 : \", total_correct1, \"correct2 : \",total_correct2)"
      ],
      "metadata": {
        "id": "IXR6dHW_8PPU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1e02d17a-fa38-4e9d-8583-004d5c63156f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch :  0 total loss :  10401.4091796875  correct1 :  595649 correct2 :  372379\n",
            "epoch :  1 total loss :  5014.6826171875  correct1 :  599665 correct2 :  501401\n",
            "epoch :  2 total loss :  3386.888427734375  correct1 :  599936 correct2 :  533625\n",
            "epoch :  3 total loss :  2402.40478515625  correct1 :  599984 correct2 :  552877\n",
            "epoch :  4 total loss :  1811.1605224609375  correct1 :  599994 correct2 :  564475\n"
          ]
        }
      ]
    }
  ]
}
